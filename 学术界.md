# CQA调研——学术界

## 目录

  * [1. 任务](#1-任务)
     * [1.1. 背景](#11-背景)
     * [1.2. 任务定义](#12-任务定义)
     * [1.3. 数据集](#13-数据集)
     * [1.4. 评测标准](#14-评测标准)
  * [2. 方法总结](#2-方法总结)
     * [2.1. 基于词频的方法](#21-基于词频的方法)
     * [2.2. 基于语义的方法](#22-基于语义的方法)
        * [2.2.1. 基于表示的方法](#221-基于表示的方法)
        * [2.2.2. 基于交互的方法](#222-基于交互的方法)
  * [3. Paper List](#3-paper-list)
     * [3.1. 论文列表](#31-论文列表)
     * [3.2. 论文解读](#32-论文解读)

## 1. 任务

### 1.1. 背景
基于问题答案对的问答系统主要涉及CQA（community question answering）与FAQ（Frequently asked questions）两种类型。网络上出现的社区问答（community question answering, CQA）提供了大规模的用户交互衍生的问题答案对（question-answer pair, QA pair）数据，为基于问答对的问答系统提供了稳定可靠的问答数据来源。与CQA相比，FAQ具有限定领域、质量高、组织好等优点，使得系统回答问题的水平大大提高。但FAQ的获取成本高，这个缺点又制约了基于FAQ的问答系统的应用范围。
### 1.2. 任务定义
对于FAQ问答和CQA问答而言，该任务通常可以可以分为两个子任务：
问题-问题匹配: 给定一个问题和一个候选问题集合,选择与其最相似的问题
问题-答案匹配：给一个问题和一个候选答案集合，选择与其最相似的答案


### 1.3. 数据集

#### QQ匹配:
Quora pairs:是Quora发布的一个数据集，旨在识别重复的问题。它由Quora上超过40万对问题组成，每个问题对都用0/1标签标注是否为重复问题。
Semeval2016/2017 task3 subtask-b:包含两个子任务，即问题-评论相似度和问题-问题相似度。在“问题-评论相似性”任务中，从问题-评论对的集合中给出问题，然后根据评论与问题的相关性对评论进行排名。在“问题-问题相似性”任务中，给定新问题，对搜索引擎检索到的所有相似问题进行重新排名
LCQMRC:百度发布的一个大型中文问题匹配数据集，数据来自百度知道。每条数据为两个问题和它们的相似性标签（用1/0代表相似/不相似)。

#### QA
Wikiqa:一组公开可用的问题答案对集合，由Microsoft Research收集和注释以用于开放域答案选择问题的研究。
TrecQA:从TRECQA8-13的数据中搜集整理的数据集，从每个问题的文档库中自动选择候选答案。该数据集是答案句子选择使用最广泛的基准之一。
InsuranceQA:第一个在保险领域发布的答案选择数据集。内容由来自真实世界用户的问题组成，高质量的答案由深层知识的专业人员组成。 


### 1.4. 评测标准
- P@1 判断排序第一的答案是否正确
- MAP(Mean Average Precision): 评测整个排序的质量。单个主题的平均准确率是每篇相关文档检索出后的准确率的平均值。
- MRR(Mean Reciprocal Rank)：是把标准答案在被评价系统给出结果中的排序取倒数作为它的准确度，再对所有的问题取平均
- NDCG(Normalized Discounted Cumulative Gain)：另一个评测排序质量的指标。

## 2. 方法总结
可以初步划分为两类，基于词频的方法，基于语义的方法

### 2.1. 基于词频的方法
在机器学习出现之前，传统文本匹配方法通常是根据句子中的词频信息进行检索的，如信息检索中的TF-IDF,BM25，向量空间模型VSM等方法，主要解决字面相似度问题。这些方法由于计算简单，适用范围广，到现在依旧是很多场景下的优秀基准模型。
#### TF-IDF介绍
TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文档集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文档与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜寻引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。

### 2.2 基于语义的方法
#### 2.2.1 基于表示的方法
基于表示的方法已被用于文本匹配任务,包括语义相似性,重复问题检测,自然语言推理。下图显示了基于表示的方法的一般架构。输入句子的矢量表示由编码器分别构建。两个输入句子对彼此表示的计算没有影响。之后，使用余弦相似度，逐元素运算或基于神经网络的组合等方法对编码的向量进行比较。这种体系结构的优势在于，将相同的编码器应用于每个输入语句会使模型更小。另外，句子向量可以用于可视化，句子聚类和许多其他目的。
将深度学习应用于答案选择的最初尝试之一是提出的词袋模型。该模型通过简单地获取句子中所有单词向量的平均值（先前已删除所有停用词）来生成句子的向量表示。与许多需要大量手工制作的功能或外部资源的传统技术相比，将其他重叠的字数统计功能与模型集成在一起可以提高性能。 QA-LSTM模型采用双向长期短期记忆（biLSTM）网络和池化层来独立构建输入句子的分布式矢量表示。然后，该模型利用余弦相似度来衡量句子表示的距离。 Severyn和Moschitti等于2015年提出了一个基于CNN的模型，该模型采用卷积神经网络（CNN）生成输入句子的表示形式。 CNN基于先前已应用于许多句子分类任务的架构。该模型中，每个输入句子都使用CNN进行建模，该CNN在多个粒度级别上提取特征并使用多种类型的池化。然后，使用多个相似性指标以几种粒度比较输入语句的表示形式。最后，将比较结果输入到完全连接的层中，以获得最终的相关性得分。
![image](https://github.com/BDBC-KG-NLP/CQA-Survey/tree/master/images/基于表示的方法)
#### 2.2.2 基于注意力的方法
在基于表示的方法中，首先将输入语句分别编码为固定长度的矢量表示形式，然后将这些表示形式进行比较。尽管其概念简单，但缺点是在编码过程中输入句子之间没有显式交互。无论考虑的候选句子是什么，问题总是映射到相同的向量，反之亦然。注意机制已被应用以减轻该弱点。 QA-LSTM-attention模型采用biLSTM网络和池化层来生成问题表示形式。候选表示的计算方法类似，只是在合并层之前，每个biLSTM输出向量都将乘以权重，该权重由问题表示形式确定。从概念上讲，注意力机制对候选答案中的某些单词赋予更多权重，并且根据问题信息计算权重。在这种情况下，注意力机制仅在单个方向上执行。
![image](https://github.com/BDBC-KG-NLP/CQA-Survey/tree/master/images/基于注意力的方法.png)
### 2.2.3 基于交互的方法
近期最先进的模型的一个共同特点是使用基于比较的方法。在这种架构下，比较输入句子的较小单位（例如单词）。然后将比较结果汇总（例如，通过CNN或RNN），以做出最终决定。下图显示了一个典型的基于比较的方法的模型-BiMPM。该模型包括以下五层。
单词表示层。该层的目标是用d维向量表示输入句子中的每个单词。 BiMPM构造具有两个分量的d维向量：一个字符组成的嵌入和一个预先用GloVe或word2vec训练的词嵌入。
上下文表示层：该层的目标是为输入句子中的每个位置获取一个新的表示形式，该表示形式除了捕获该位置的单词以外，还捕获一些上下文信息。 BiMPM使用biLSTM生成上下文表示。
匹配层：该层的目标是将一个句子的每个上下文表示与另一句子的所有上下文表示进行比较。该层的输出是两个匹配向量序列，其中每个匹配向量对应于一个句子的一个位置与另一个句子的所有位置的比较结果。
聚合层：该层的目标是汇总来自上一层的比较结果。 BiMPM使用另一个BiLSTM将匹配向量的两个序列聚合为固定长度向量。
预测层：该层的目标是做出最终预测。 BiMPM使用两层前馈神经网络来消耗前一层的固定长度矢量，并应用softmax函数获得最终分数。
与基于表示的方法和基于注意力的方法相比，基于比较的方法可以捕获输入句子之间的更多交互功能，因此在对TrecQA等公共数据集进行评估时，通常具有更好的性能。
![image](https://github.com/BDBC-KG-NLP/CQA-Survey/tree/master/images/基于比较的方法.png)

### 3.1. 论文列表
| 会议/年份  | 论文 |链接|
| ------------- | ------------- |------------- |
|CIKM2013   |Learning Deep Structured Semantic Models for Web Search using Clickthrough Data|https://dl.acm.org/doi/10.1145/2505515.2505665 |
|CIKM2016   |A Deep Relevance Matching Model for Ad-hoc Retrieval|https://arxiv.org/abs/1711.08611  |
|KDD2018    |Multi-Cast Attention Networks for Retrieval-based Question Answering and Response Prediction   |https://arxiv.org/pdf/1806.00778.pdf  |
|ACL2018    |Enhanced LSTM for Natural Language Inference   |https://arxiv.org/pdf/1609.06038.pdf |
|CIKM2019   |A Compare-Aggregate Model with Latent Clustering for Answer Selection|https://arxiv.org/abs/1905.12897|
|ACL2019    |Simple and Effective Text Matching with Richer Alignment Features|https://www.aclweb.org/anthology/P19-1465/ |
|WWW2019    |A Hierarchical Attention Retrieval Model for Healthcare Question Answering   |http://dmkd.cs.vt.edu/papers/WWW19.pdf |
|IJCAI2019  |Multiway Attention Networks for Modeling Sentences Pairs | https://www.ijcai.org/Proceedings/2018/0613.pdf |
|NAACL2019  |Alignment over Heterogeneous Embeddings for Question Answering |https://www.aclweb.org/anthology/N19-1274/ |
|AAAI2019   |DRr-Net: Dynamic Re-Read Network for Sentence Semantic Matching | https://www.aaai.org/ojs/index.php/AAAI/article/view/4734/4612 |
|NAACL2019  |BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|https://arxiv.org/abs/1810.04805   |
|ICLR2019   |QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension|https://arxiv.org/abs/1804.09541   |
|ICLR2020   |ALBERT: A Lite BERT for Self-supervised Learning of Language Representations|https://arxiv.org/abs/1909.11942   |
|AAAI2020   |TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection|https://arxiv.org/abs/1911.04118    |

### 3.2. 论文解读
>《DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching》
![image](https://github.com/BDBC-KG-NLP/CQA-Survey/tree/master/images/dr-net.png)

介绍：语义匹配一直是一项十分重要的任务，目前，注意力机制大大提升了语义匹配的效果。不过过去的注意力机制通常是一次性关注所有关键词，而人类阅读过程中对关键词的注意往往是变化的。为此，本文提出了一种动态关注关键词的模型
模型：
整个模型可以分为三个部分：输入，动态选择关注词，分类
1. 输入（encode）
初始输入为词向量拼接字符级别的词向量以及手工特征（pos，exact match），用一个简单的线性层作一个变换。之后将输入送入一个stack-gru中，即下一层的输入为上一层的输入拼接原始输入（类似残差网络）。
最终，通过一个self-attention将输出的加权和作为句子的表示，文中成为original represtion。
2. 动态重读（ Dynamic Re-read Mechanism）
利用一个注意力机制根据句子的表示，上一次选择的关键词，选择此次的关键词，送入一个gru学习。
3. 分类
对原始表示，重读后的表示，分别拼接表示向量，element-wise的乘积与差，用一个线性层训练。并且动态加权。

>《Simple and Effective Text Matching with Richer Alignment Features》
![image](https://github.com/BDBC-KG-NLP/CQA-Survey/tree/master/images/re2.png)

介绍：提出了一种简单的，不存在复杂特殊结构的文本匹配模型，主要通过point wise信息，上下文相关信息，和前一层提取的相关性信息的结合来表示文档的相关性。
模型：
模型为多层类似结构的组合，对两个输入句子采用完全对称的处理。每层的输入为上一层输出与原始embedding的拼接。每一层的输出部分要与上一层的输出相加，每层结构由embedding，encoder和fusion三部分族中。最终将最后一层的输出经过池化后利用predicter部分（一个多层的神经网络）计算最终结果。
embedding部分
采用glove作为初始embeeding，每层的输入为上一层输出与原始embedding的拼接。
encoder部分
文中采用了一个cnn作为encoder结构，将encoder输出与encoder的输入拼接，作为每个单词的表示，通过计算内积，得到两个句子attention相关的表示。
fusion部分
通过两个句子encoder输出的差，点积和拼接等，通过线性变换得到新的表示。

## 4. 参考资料

https://zhuanlan.zhihu.com/p/98688910
https://github.com/NTMC-Community/awesome-neural-models-for-semantic-match